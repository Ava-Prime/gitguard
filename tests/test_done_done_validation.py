#!/usr/bin/env python3
"""
Test script for "Done-Done" validation checklist.
Validates all core features are working correctly per RUNBOOK.md.
"""


def test_policy_transparency():
    """Test that policy source & inputs render on PR pages."""
    print("\nüîç Testing Policy Transparency...")

    # Check for policy_explain.py module
    policy_file = "apps/guard-codex/policy_explain.py"

    try:
        with open(policy_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def explain_policy_decision", "Policy explanation function"),
            ("load_rego_snippet", "Rule source extraction"),
            ("opa_inputs", "Input data capture"),
            ("render_policy_block", "Decision explanation"),
            ("Policy Evaluation", "Section header"),
        ]

        print("üìã Policy transparency checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"‚ùå Policy module not found: {policy_file}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading policy module: {e}")
        return False


def test_mermaid_graphs():
    """Test that Mermaid graphs appear on PR pages (‚â§20 nodes)."""
    print("\nüîç Testing Mermaid Graph Integration...")

    activities_file = "apps/guard-codex/activities.py"

    try:
        with open(activities_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def _mermaid", "Mermaid function definition"),
            ("graph LR", "Mermaid graph syntax"),
            ("touches", "File relationships"),
            ("[:20]", "Node count limiting"),
            ("cap to keep it readable", "Node limit documentation"),
        ]

        print("üìã Mermaid graph checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"‚ùå Activities file not found: {activities_file}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading activities file: {e}")
        return False


def test_owners_index():
    """Test that owners index updates as part of render cycle."""
    print("\nüîç Testing Owners Index Integration...")

    owners_file = "apps/guard-codex/owners_emit.py"

    try:
        with open(owners_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def emit_owners_index", "Owners index function"),
            ("db_url", "Database integration"),
            ("owners.md", "Output file generation"),
            ("from graph data", "Integration documentation"),
        ]

        print("üìã Owners index checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"‚ùå Owners module not found: {owners_file}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading owners module: {e}")
        return False


def test_freshness_alert():
    """Test that freshness P99 alert exists in Prometheus."""
    print("\nüîç Testing Freshness SLO Alert...")

    # Check activities.py for DOC_FRESH metric
    activities_file = "apps/guard-codex/activities.py"
    alerts_file = "ops/prometheus/alerts_codex.yml"

    try:
        # Check metric implementation
        with open(activities_file, encoding="utf-8") as f:
            activities_content = f.read()

        # Check alert configuration
        with open(alerts_file, encoding="utf-8") as f:
            alerts_content = f.read()

        checks = [
            (activities_content, "DOC_FRESH", "Histogram metric"),
            (activities_content, "DOC_FRESH.observe", "Metric observation"),
            (alerts_content, "CodexFreshnessSLOBreached", "Alert rule"),
            (alerts_content, "histogram_quantile(0.99", "P99 calculation"),
            (alerts_content, "> 180", "SLO threshold"),
        ]

        print("üìã Freshness SLO checks:")
        all_passed = True
        for content, check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError as e:
        print(f"‚ùå File not found: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading files: {e}")
        return False


def test_chaos_drill():
    """Test that chaos slow-publish drill exists and works."""
    print("\nüîç Testing Chaos Engineering Drill...")

    makefile_path = "Makefile"

    try:
        with open(makefile_path, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("chaos.slow-publish:", "Chaos drill target"),
            ("FAULT_ONCE_DELIVERY_ID=ALERT-TEST", "Fault injection"),
            ("PUBLISH_SLOW_MS=90000", "Slow publish simulation"),
            ("CodexBuildStall alert", "Alert testing"),
            ("JetStream redelivery", "Redelivery testing"),
        ]

        print("üìã Chaos drill checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"‚ùå Makefile not found: {makefile_path}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading Makefile: {e}")
        return False


def test_runbook_exists():
    """Test that RUNBOOK.md exists with all required sections."""
    print("\nüîç Testing Runbook Documentation...")

    runbook_path = "RUNBOOK.md"

    try:
        with open(runbook_path, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("# GitGuard Codex Operations Runbook", "Main title"),
            ("Docs Drift: Owners Index Out of Sync", "Docs drift section"),
            ("Policy Confusion: Engineers Need Rule Clarity", "Policy section"),
            ("JetStream Stuck: Message Processing Lag", "JetStream section"),
            ('"Done-Done" Validation Checklist', "Validation checklist"),
            ("Policy Source & Inputs Render", "Policy validation"),
            ("Mermaid Graph Appears", "Mermaid validation"),
            ("Owners Index Updates", "Owners validation"),
            ("Freshness P99 Alert", "Freshness validation"),
            ("Chaos Slow-Publish Drill", "Chaos validation"),
        ]

        print("üìã Runbook sections:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   ‚úÖ {description}")
            else:
                print(f"   ‚ùå {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"‚ùå Runbook not found: {runbook_path}")
        return False
    except Exception as e:
        print(f"‚ùå Error reading runbook: {e}")
        return False


def explain_org_brain():
    """Explain the org-brain concept and its capabilities."""
    print("\nüí° Org-Brain: Documentation with Judgment & Receipts")
    print("=" * 60)

    print("\nüß† What Makes This an 'Org-Brain':")
    print("   ‚Ä¢ Every decision shows its source code and reasoning")
    print("   ‚Ä¢ Visual graphs reveal hidden relationships")
    print("   ‚Ä¢ Always-current ownership information")
    print("   ‚Ä¢ Self-monitoring with SLO alerts")
    print("   ‚Ä¢ Chaos testing validates resilience")
    print("   ‚Ä¢ API access enables future integrations")

    print("\nüìä The Line Between Documentation & Judgment:")
    print("   Traditional Docs: 'This is how it works'")
    print("   Org-Brain: 'This is how it works, here's the proof'")

    print("\nüéØ Core Capabilities Delivered:")
    print("   1. Policy Transparency - Show exact rules & inputs")
    print("   2. Visual Relationships - Mermaid graphs reveal connections")
    print("   3. Ownership Clarity - Always-current owners index")
    print("   4. Performance Monitoring - SLO alerts ensure health")
    print("   5. Chaos Resilience - Automated failure testing")
    print("   6. API Access - Graph endpoint for integrations")

    print("\nüöÄ Future Extensions Ready:")
    print("   ‚Ä¢ Tiny D3 graphs on PR pages")
    print("   ‚Ä¢ Incident timelines auto-linking to fixing PRs")
    print("   ‚Ä¢ Real-time collaboration features")
    print("   ‚Ä¢ Advanced analytics and insights")

    print("\n‚ú® The Result:")
    print("   An organizational knowledge system that can explain")
    print("   every decision it makes, with full transparency and")
    print("   traceability. This is judgment with receipts.")


def run_all_tests():
    """Run all done-done validation tests."""
    print("üö® GitGuard Codex: Done-Done Validation")
    print("=" * 50)

    # Run all tests
    tests = [
        ("Policy Transparency", test_policy_transparency),
        ("Mermaid Graphs", test_mermaid_graphs),
        ("Owners Index", test_owners_index),
        ("Freshness Alert", test_freshness_alert),
        ("Chaos Drill", test_chaos_drill),
        ("Runbook Documentation", test_runbook_exists),
    ]

    results = {}
    for test_name, test_func in tests:
        results[test_name] = test_func()

    # Show org-brain explanation
    explain_org_brain()

    # Summary
    print("\n" + "=" * 60)
    print("üìã Done-Done Validation Summary:")

    all_passed = True
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"   {test_name}: {status}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\nüéâ ALL DONE-DONE CRITERIA MET!")
        print("\nüö® System Status: PRODUCTION READY")
        print("\n‚ú® You've built an org-brain that can explain itself.")
        print("   This is the line between 'documentation' and")
        print("   'judgment with receipts'.")

        print("\nüöÄ Ready for Extensions:")
        print("   ‚Ä¢ Tiny D3 graphs on PR pages")
        print("   ‚Ä¢ Incident timelines with PR auto-linking")
        print("   ‚Ä¢ Advanced analytics and insights")
        print("   ‚Ä¢ Real-time collaboration features")
    else:
        print("\n‚ùå Some criteria not met. Check implementation.")
        print("\nüìñ Refer to RUNBOOK.md for troubleshooting.")


if __name__ == "__main__":
    run_all_tests()
