#!/usr/bin/env python3
"""
Test script for "Done-Done" validation checklist.
Validates all core features are working correctly per RUNBOOK.md.
"""


def test_policy_transparency():
    """Test that policy source & inputs render on PR pages."""
    print("\nğŸ” Testing Policy Transparency...")

    # Check for policy_explain.py module
    policy_file = "apps/guard-codex/policy_explain.py"

    try:
        with open(policy_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def explain_policy_decision", "Policy explanation function"),
            ("load_rego_snippet", "Rule source extraction"),
            ("opa_inputs", "Input data capture"),
            ("render_policy_block", "Decision explanation"),
            ("Policy Evaluation", "Section header"),
        ]

        print("ğŸ“‹ Policy transparency checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"âŒ Policy module not found: {policy_file}")
        return False
    except Exception as e:
        print(f"âŒ Error reading policy module: {e}")
        return False


def test_mermaid_graphs():
    """Test that Mermaid graphs appear on PR pages (â‰¤20 nodes)."""
    print("\nğŸ” Testing Mermaid Graph Integration...")

    activities_file = "apps/guard-codex/activities.py"

    try:
        with open(activities_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def _mermaid", "Mermaid function definition"),
            ("graph LR", "Mermaid graph syntax"),
            ("touches", "File relationships"),
            ("[:20]", "Node count limiting"),
            ("cap to keep it readable", "Node limit documentation"),
        ]

        print("ğŸ“‹ Mermaid graph checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"âŒ Activities file not found: {activities_file}")
        return False
    except Exception as e:
        print(f"âŒ Error reading activities file: {e}")
        return False


def test_owners_index():
    """Test that owners index updates as part of render cycle."""
    print("\nğŸ” Testing Owners Index Integration...")

    owners_file = "apps/guard-codex/owners_emit.py"

    try:
        with open(owners_file, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("def emit_owners_index", "Owners index function"),
            ("db_url", "Database integration"),
            ("owners.md", "Output file generation"),
            ("from graph data", "Integration documentation"),
        ]

        print("ğŸ“‹ Owners index checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"âŒ Owners module not found: {owners_file}")
        return False
    except Exception as e:
        print(f"âŒ Error reading owners module: {e}")
        return False


def test_freshness_alert():
    """Test that freshness P99 alert exists in Prometheus."""
    print("\nğŸ” Testing Freshness SLO Alert...")

    # Check activities.py for DOC_FRESH metric
    activities_file = "apps/guard-codex/activities.py"
    alerts_file = "ops/prometheus/alerts_codex.yml"

    try:
        # Check metric implementation
        with open(activities_file, encoding="utf-8") as f:
            activities_content = f.read()

        # Check alert configuration
        with open(alerts_file, encoding="utf-8") as f:
            alerts_content = f.read()

        checks = [
            (activities_content, "DOC_FRESH", "Histogram metric"),
            (activities_content, "DOC_FRESH.observe", "Metric observation"),
            (alerts_content, "CodexFreshnessSLOBreached", "Alert rule"),
            (alerts_content, "histogram_quantile(0.99", "P99 calculation"),
            (alerts_content, "> 180", "SLO threshold"),
        ]

        print("ğŸ“‹ Freshness SLO checks:")
        all_passed = True
        for content, check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError as e:
        print(f"âŒ File not found: {e}")
        return False
    except Exception as e:
        print(f"âŒ Error reading files: {e}")
        return False


def test_chaos_drill():
    """Test that chaos slow-publish drill exists and works."""
    print("\nğŸ” Testing Chaos Engineering Drill...")

    makefile_path = "Makefile"

    try:
        with open(makefile_path, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("chaos.slow-publish:", "Chaos drill target"),
            ("FAULT_ONCE_DELIVERY_ID=ALERT-TEST", "Fault injection"),
            ("PUBLISH_SLOW_MS=90000", "Slow publish simulation"),
            ("CodexBuildStall alert", "Alert testing"),
            ("JetStream redelivery", "Redelivery testing"),
        ]

        print("ğŸ“‹ Chaos drill checks:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"âŒ Makefile not found: {makefile_path}")
        return False
    except Exception as e:
        print(f"âŒ Error reading Makefile: {e}")
        return False


def test_runbook_exists():
    """Test that RUNBOOK.md exists with all required sections."""
    print("\nğŸ” Testing Runbook Documentation...")

    runbook_path = "RUNBOOK.md"

    try:
        with open(runbook_path, encoding="utf-8") as f:
            content = f.read()

        checks = [
            ("# GitGuard Codex Operations Runbook", "Main title"),
            ("Docs Drift: Owners Index Out of Sync", "Docs drift section"),
            ("Policy Confusion: Engineers Need Rule Clarity", "Policy section"),
            ("JetStream Stuck: Message Processing Lag", "JetStream section"),
            ('"Done-Done" Validation Checklist', "Validation checklist"),
            ("Policy Source & Inputs Render", "Policy validation"),
            ("Mermaid Graph Appears", "Mermaid validation"),
            ("Owners Index Updates", "Owners validation"),
            ("Freshness P99 Alert", "Freshness validation"),
            ("Chaos Slow-Publish Drill", "Chaos validation"),
        ]

        print("ğŸ“‹ Runbook sections:")
        all_passed = True
        for check, description in checks:
            if check in content:
                print(f"   âœ… {description}")
            else:
                print(f"   âŒ {description}")
                all_passed = False

        return all_passed

    except FileNotFoundError:
        print(f"âŒ Runbook not found: {runbook_path}")
        return False
    except Exception as e:
        print(f"âŒ Error reading runbook: {e}")
        return False


def explain_org_brain():
    """Explain the org-brain concept and its capabilities."""
    print("\nğŸ’¡ Org-Brain: Documentation with Judgment & Receipts")
    print("=" * 60)

    print("\nğŸ§  What Makes This an 'Org-Brain':")
    print("   â€¢ Every decision shows its source code and reasoning")
    print("   â€¢ Visual graphs reveal hidden relationships")
    print("   â€¢ Always-current ownership information")
    print("   â€¢ Self-monitoring with SLO alerts")
    print("   â€¢ Chaos testing validates resilience")
    print("   â€¢ API access enables future integrations")

    print("\nğŸ“Š The Line Between Documentation & Judgment:")
    print("   Traditional Docs: 'This is how it works'")
    print("   Org-Brain: 'This is how it works, here's the proof'")

    print("\nğŸ¯ Core Capabilities Delivered:")
    print("   1. Policy Transparency - Show exact rules & inputs")
    print("   2. Visual Relationships - Mermaid graphs reveal connections")
    print("   3. Ownership Clarity - Always-current owners index")
    print("   4. Performance Monitoring - SLO alerts ensure health")
    print("   5. Chaos Resilience - Automated failure testing")
    print("   6. API Access - Graph endpoint for integrations")

    print("\nğŸš€ Future Extensions Ready:")
    print("   â€¢ Tiny D3 graphs on PR pages")
    print("   â€¢ Incident timelines auto-linking to fixing PRs")
    print("   â€¢ Real-time collaboration features")
    print("   â€¢ Advanced analytics and insights")

    print("\nâœ¨ The Result:")
    print("   An organizational knowledge system that can explain")
    print("   every decision it makes, with full transparency and")
    print("   traceability. This is judgment with receipts.")


def run_all_tests():
    """Run all done-done validation tests."""
    print("ğŸš¨ GitGuard Codex: Done-Done Validation")
    print("=" * 50)

    # Run all tests
    tests = [
        ("Policy Transparency", test_policy_transparency),
        ("Mermaid Graphs", test_mermaid_graphs),
        ("Owners Index", test_owners_index),
        ("Freshness Alert", test_freshness_alert),
        ("Chaos Drill", test_chaos_drill),
        ("Runbook Documentation", test_runbook_exists),
    ]

    results = {}
    for test_name, test_func in tests:
        results[test_name] = test_func()

    # Show org-brain explanation
    explain_org_brain()

    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ Done-Done Validation Summary:")

    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"   {test_name}: {status}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\nğŸ‰ ALL DONE-DONE CRITERIA MET!")
        print("\nğŸš¨ System Status: PRODUCTION READY")
        print("\nâœ¨ You've built an org-brain that can explain itself.")
        print("   This is the line between 'documentation' and")
        print("   'judgment with receipts'.")

        print("\nğŸš€ Ready for Extensions:")
        print("   â€¢ Tiny D3 graphs on PR pages")
        print("   â€¢ Incident timelines with PR auto-linking")
        print("   â€¢ Advanced analytics and insights")
        print("   â€¢ Real-time collaboration features")
    else:
        print("\nâŒ Some criteria not met. Check implementation.")
        print("\nğŸ“– Refer to RUNBOOK.md for troubleshooting.")


if __name__ == "__main__":
    run_all_tests()
